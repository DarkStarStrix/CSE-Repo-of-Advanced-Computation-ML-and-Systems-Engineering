{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11407660,"sourceType":"datasetVersion","datasetId":7145865}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports\nimport gc\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom transformers import BertTokenizer, BertForSequenceClassification, T5Tokenizer, T5ForConditionalGeneration\nimport optuna\nimport os\nimport json\nfrom tqdm import tqdm\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T20:54:10.691920Z","iopub.execute_input":"2025-04-15T20:54:10.692636Z","iopub.status.idle":"2025-04-15T20:54:10.697238Z","shell.execute_reply.started":"2025-04-15T20:54:10.692602Z","shell.execute_reply":"2025-04-15T20:54:10.696628Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Device setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nprint(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n\n# Load dataset\nwith open('/kaggle/input/moe-dataset/combined_scientific_papers.json', 'r') as f:\n    data = json.load(f)\n\n# Domain to label mapping\ndomain_to_label = {domain: idx for idx, domain in enumerate(set(entry['domain'] for entry in data))}\nnum_labels = len(domain_to_label)\n# Define num_experts based on the number of unique domains/labels\nnum_experts = num_labels\nprint(f\"Number of experts/domains: {num_experts}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T20:54:10.710059Z","iopub.execute_input":"2025-04-15T20:54:10.710322Z","iopub.status.idle":"2025-04-15T20:54:10.752228Z","shell.execute_reply.started":"2025-04-15T20:54:10.710304Z","shell.execute_reply":"2025-04-15T20:54:10.751641Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nNumber of GPUs available: 2\nNumber of experts/domains: 3\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"class ScientificDataset(Dataset):\n    def __init__(self, data, domain_to_label):\n        self.queries = [entry['text'][:100] for entry in data]\n        self.labels = [domain_to_label[entry['domain']] for entry in data]\n        self.responses = [entry['text'] for entry in data]\n    \n    def __len__(self):\n        return len(self.queries)\n    \n    def __getitem__(self, idx):\n        return self.queries[idx], self.labels[idx], self.responses[idx]\n\n# Collate functions\ndef gating_collate_fn(batch):\n    queries, labels, _ = zip(*batch)\n    tokenized = bert_tokenizer(list(queries), padding=True, truncation=True, return_tensors='pt')\n    return tokenized, torch.tensor(labels)\n\ndef expert_collate_fn(batch):\n    queries, responses = zip(*batch)\n    inputs = t5_tokenizer(list(queries), padding=True, truncation=True, return_tensors='pt')\n    targets = t5_tokenizer(list(responses), padding=True, truncation=True, return_tensors='pt')\n    return inputs, targets['input_ids']\n\n# Expert dataset\nclass TextDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx]\n\n# Extract expert data\ndef get_expert_data(dataset, expert_id):\n    return [(query, response) for query, label, response in dataset if label == expert_id]\n\n# Training functions with gradient accumulation\ndef train_gating_model(model, train_loader, val_loader, lr, epochs, accumulation_steps=4):\n    optimizer = optim.AdamW(model.parameters(), lr=lr)\n    model = nn.DataParallel(model)  # Utilize multiple GPUs\n    model.to(device)\n    loss_fct = nn.CrossEntropyLoss()\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        optimizer.zero_grad()\n        for i, batch in enumerate(train_loader):\n            inputs, labels = batch\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            labels = labels.to(device)\n            # Call the model without labels to get logits\n            outputs = model(input_ids=inputs['input_ids'], \n                            attention_mask=inputs['attention_mask'])\n            logits = outputs.logits\n            loss = loss_fct(logits, labels) / accumulation_steps\n            loss.backward()\n            if (i + 1) % accumulation_steps == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n            total_loss += loss.item() * accumulation_steps\n        avg_loss = total_loss / len(train_loader)\n        print(f\"Gating Epoch {epoch+1}, Train Loss: {avg_loss:.4f}\")\n        gc.collect()\n        torch.cuda.empty_cache()\n    return model\n\ndef train_expert(model, train_loader, val_loader, lr, epochs, expert_id, accumulation_steps=4):\n    optimizer = optim.AdamW(model.parameters(), lr=lr)\n    model = nn.DataParallel(model)  # Utilize multiple GPUs\n    model.to(device)\n    pad_token_id = model.module.config.pad_token_id  # Access underlying model's config\n    loss_fct = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        optimizer.zero_grad()\n        for i, batch in enumerate(train_loader):\n            inputs, labels = batch\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            labels = labels.to(device)\n            # Prepare decoder input and target labels\n            decoder_input_ids = labels[:, :-1].clone()\n            target_labels = labels[:, 1:].clone()\n            # Call model without labels to get logits\n            outputs = model(input_ids=inputs['input_ids'], \n                            attention_mask=inputs['attention_mask'], \n                            decoder_input_ids=decoder_input_ids)\n            logits = outputs.logits\n            # Compute loss\n            loss = loss_fct(logits.view(-1, logits.size(-1)), target_labels.view(-1)) / accumulation_steps\n            loss.backward()\n            if (i + 1) % accumulation_steps == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n            total_loss += loss.item() * accumulation_steps\n        avg_loss = total_loss / len(train_loader)\n        print(f\"Expert {expert_id} Epoch {epoch+1}, Train Loss: {avg_loss:.4f}\")\n        gc.collect()\n        torch.cuda.empty_cache()\n    return model\n\n# Evaluation functions\ndef evaluate_gating_model(model, val_loader):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            inputs, labels = batch\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            labels = labels.to(device)\n            # Explicitly specify input parameters\n            outputs = model(input_ids=inputs['input_ids'], \n                           attention_mask=inputs['attention_mask'])\n            _, predicted = torch.max(outputs.logits, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return correct / total\n\ndef evaluate_expert(model, val_loader):\n    if len(val_loader) == 0:\n        return 0.0  # Return 0 loss if validation loader is empty\n    model.eval()\n    total_loss = 0\n    pad_token_id = model.module.config.pad_token_id\n    loss_fct = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n    with torch.no_grad():\n        for batch in val_loader:\n            inputs, labels = batch\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            labels = labels.to(device)\n            decoder_input_ids = labels[:, :-1].clone()\n            target_labels = labels[:, 1:].clone()\n            outputs = model(input_ids=inputs['input_ids'], \n                            attention_mask=inputs['attention_mask'], \n                            decoder_input_ids=decoder_input_ids)\n            logits = outputs.logits\n            loss = loss_fct(logits.view(-1, logits.size(-1)), target_labels.view(-1))\n            total_loss += loss.item()\n    return total_loss / len(val_loader)\n\n# Optuna objective\ndef objective(trial):\n    # Hyperparameter suggestions\n    gating_lr = trial.suggest_float('gating_lr', 1e-5, 1e-3, log=True)\n    gating_epochs = trial.suggest_int('gating_epochs', 3, 10)\n    expert_lr = trial.suggest_float('expert_lr', 1e-5, 1e-3, log=True)\n    expert_epochs = trial.suggest_int('expert_epochs', 3, 10)\n    \n    # Load and train gating model\n    gating_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_experts).to(device)\n    gating_model = train_gating_model(gating_model, train_loader_gating, val_loader_gating, gating_lr, gating_epochs)\n    gating_accuracy = evaluate_gating_model(gating_model, val_loader_gating)\n    gating_model.to('cpu')  # Move gating model to CPU\n    torch.cuda.empty_cache()  # Clear GPU memory\n    \n    # Initialize experts dictionary\n    experts = {}\n    expert_losses = []\n    \n    # Train each expert sequentially\n    for expert_id in range(num_experts):\n        train_data = train_expert_data[expert_id]\n        val_data = val_expert_data[expert_id]\n        if not train_data:\n            continue\n        # Load expert model onto GPU\n        expert_model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n        train_dataset_expert = TextDataset(train_data)\n        val_dataset_expert = TextDataset(val_data)\n        train_loader_expert = DataLoader(train_dataset_expert, batch_size=2, shuffle=True, collate_fn=expert_collate_fn)\n        val_loader_expert = DataLoader(val_dataset_expert, batch_size=2, shuffle=False, collate_fn=expert_collate_fn)\n        expert_model = train_expert(expert_model, train_loader_expert, val_loader_expert, expert_lr, expert_epochs, expert_id)\n        loss = evaluate_expert(expert_model, val_loader_expert)\n        expert_losses.append(loss)\n        expert_model.to('cpu')  # Move expert model to CPU\n        experts[expert_id] = expert_model  # Store on CPU\n        torch.cuda.empty_cache()  # Clear GPU memory\n    \n    # Compute average expert loss\n    avg_expert_loss = sum(expert_losses) / len(expert_losses) if expert_losses else 0\n    \n    # Compute combined metric\n    combined_metric = gating_accuracy - 0.1 * avg_expert_loss\n    \n    # Set user attributes (models are already on CPU)\n    trial.set_user_attr('gating_accuracy', gating_accuracy)\n    trial.set_user_attr('avg_expert_loss', avg_expert_loss)\n    trial.set_user_attr('experts', experts)\n    trial.set_user_attr('gating_model', gating_model)\n    \n    return combined_metric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T20:54:10.753361Z","iopub.execute_input":"2025-04-15T20:54:10.753578Z","iopub.status.idle":"2025-04-15T20:54:10.775759Z","shell.execute_reply.started":"2025-04-15T20:54:10.753561Z","shell.execute_reply":"2025-04-15T20:54:10.775040Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Setup dataset and dataloaders\n    full_dataset = ScientificDataset(data, domain_to_label)\n    train_size = int(0.8 * len(full_dataset))\n    val_size = len(full_dataset) - train_size\n    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n    \n    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    t5_tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False)\n    \n    train_loader_gating = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=gating_collate_fn)\n    val_loader_gating = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=gating_collate_fn)\n    \n    train_expert_data = {i: get_expert_data(train_dataset, i) for i in range(num_experts)}\n    val_expert_data = {i: get_expert_data(val_dataset, i) for i in range(num_experts)}\n    \n    # Run Optuna study\n    study = optuna.create_study(direction='maximize')\n    study.optimize(objective, n_trials=20)\n    \n    # Benchmark and compare\n    print(\"\\nBenchmarking Top 5 Configurations:\")\n    top_trials = sorted(study.trials, key=lambda t: t.value, reverse=True)[:5]\n    for i, trial in enumerate(top_trials):\n        print(f\"\\nMOE {i+1}:\")\n        print(f\"  Trial Number: {trial.number}\")\n        print(f\"  Combined Metric: {trial.value:.4f}\")\n        print(f\"  Gating Accuracy: {trial.user_attrs['gating_accuracy']:.4f}\")\n        print(f\"  Avg Expert Loss: {trial.user_attrs['avg_expert_loss']:.4f}\")\n        print(f\"  Hyperparameters: {trial.params}\")\n    \n    # Save top 5 MOE models\n    for i, trial in enumerate(top_trials):\n        moe_dir = f'MOE_{i+1}'\n        os.makedirs(moe_dir, exist_ok=True)\n        \n        gating_model = trial.user_attrs['gating_model']\n        experts = trial.user_attrs['experts']\n        \n        torch.save(gating_model.state_dict(), os.path.join(moe_dir, 'gating.pt'))\n        for expert_id, expert in experts.items():\n            torch.save(expert.state_dict(), os.path.join(moe_dir, f'expert_{expert_id}.pt'))\n        \n        metrics = {\n            'gating_accuracy': trial.user_attrs['gating_accuracy'],\n            'avg_expert_loss': trial.user_attrs['avg_expert_loss'],\n            'combined_metric': trial.value\n        }\n        with open(os.path.join(moe_dir, 'metrics.json'), 'w') as f:\n            json.dump(metrics, f)\n        with open(os.path.join(moe_dir, 'hyperparams.json'), 'w') as f:\n            json.dump(trial.params, f)\n    \n    print(\"\\nTop 5 MOE models saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T20:54:10.776896Z","iopub.execute_input":"2025-04-15T20:54:10.777145Z","execution_failed":"2025-04-15T20:59:40.639Z"}},"outputs":[{"name":"stderr","text":"[I 2025-04-15 20:54:11,575] A new study created in memory with name: no-name-dca24d30-514f-4b88-a086-0900ac680d04\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.1737\nGating Epoch 2, Train Loss: 1.0351\nGating Epoch 3, Train Loss: 1.0402\nGating Epoch 4, Train Loss: 1.0824\nGating Epoch 5, Train Loss: 1.2443\nExpert 0 Epoch 1, Train Loss: 8.6673\nExpert 0 Epoch 2, Train Loss: 7.5167\nExpert 0 Epoch 3, Train Loss: 6.6618\nExpert 0 Epoch 4, Train Loss: 6.6367\nExpert 0 Epoch 5, Train Loss: 6.2782\nExpert 1 Epoch 1, Train Loss: 9.0753\nExpert 1 Epoch 2, Train Loss: 8.4852\nExpert 1 Epoch 3, Train Loss: 8.7325\nExpert 1 Epoch 4, Train Loss: 7.8988\nExpert 1 Epoch 5, Train Loss: 8.8625\nExpert 2 Epoch 1, Train Loss: 8.5271\nExpert 2 Epoch 2, Train Loss: 8.5603\nExpert 2 Epoch 3, Train Loss: 8.0651\nExpert 2 Epoch 4, Train Loss: 8.1677\nExpert 2 Epoch 5, Train Loss: 8.6763\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 20:54:34,592] Trial 0 finished with value: -0.32766657670338944 and parameters: {'gating_lr': 1.101847007202224e-05, 'gating_epochs': 5, 'expert_lr': 8.98230572282742e-05, 'expert_epochs': 5}. Best is trial 0 with value: -0.32766657670338944.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.2531\nGating Epoch 2, Train Loss: 0.9784\nGating Epoch 3, Train Loss: 1.0459\nGating Epoch 4, Train Loss: 0.9803\nGating Epoch 5, Train Loss: 1.3713\nGating Epoch 6, Train Loss: 1.3111\nGating Epoch 7, Train Loss: 1.3390\nGating Epoch 8, Train Loss: 1.3813\nExpert 0 Epoch 1, Train Loss: 9.4270\nExpert 0 Epoch 2, Train Loss: 7.0293\nExpert 0 Epoch 3, Train Loss: 6.6949\nExpert 0 Epoch 4, Train Loss: 6.3036\nExpert 0 Epoch 5, Train Loss: 6.1333\nExpert 0 Epoch 6, Train Loss: 6.0103\nExpert 0 Epoch 7, Train Loss: 6.0199\nExpert 0 Epoch 8, Train Loss: 5.8100\nExpert 0 Epoch 9, Train Loss: 5.8444\nExpert 0 Epoch 10, Train Loss: 5.7876\nExpert 1 Epoch 1, Train Loss: 8.5045\nExpert 1 Epoch 2, Train Loss: 7.9095\nExpert 1 Epoch 3, Train Loss: 8.4441\nExpert 1 Epoch 4, Train Loss: 8.7526\nExpert 1 Epoch 5, Train Loss: 9.1628\nExpert 1 Epoch 6, Train Loss: 8.4946\nExpert 1 Epoch 7, Train Loss: 8.1799\nExpert 1 Epoch 8, Train Loss: 8.7275\nExpert 1 Epoch 9, Train Loss: 8.4489\nExpert 1 Epoch 10, Train Loss: 8.9391\nExpert 2 Epoch 1, Train Loss: 8.4343\nExpert 2 Epoch 2, Train Loss: 7.7678\nExpert 2 Epoch 3, Train Loss: 8.8971\nExpert 2 Epoch 4, Train Loss: 7.9069\nExpert 2 Epoch 5, Train Loss: 7.6573\nExpert 2 Epoch 6, Train Loss: 8.5016\nExpert 2 Epoch 7, Train Loss: 8.5880\nExpert 2 Epoch 8, Train Loss: 8.9373\nExpert 2 Epoch 9, Train Loss: 9.0575\nExpert 2 Epoch 10, Train Loss: 9.4115\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 20:55:14,473] Trial 1 finished with value: -0.32766657670338944 and parameters: {'gating_lr': 0.00022329585625252254, 'gating_epochs': 8, 'expert_lr': 0.00011410160679719041, 'expert_epochs': 10}. Best is trial 0 with value: -0.32766657670338944.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.1637\nGating Epoch 2, Train Loss: 1.1620\nGating Epoch 3, Train Loss: 1.1692\nGating Epoch 4, Train Loss: 1.1596\nGating Epoch 5, Train Loss: 1.2054\nGating Epoch 6, Train Loss: 1.1765\nGating Epoch 7, Train Loss: 1.1189\nGating Epoch 8, Train Loss: 1.1145\nGating Epoch 9, Train Loss: 1.0725\nExpert 0 Epoch 1, Train Loss: 8.5783\nExpert 0 Epoch 2, Train Loss: 8.2485\nExpert 0 Epoch 3, Train Loss: 7.4153\nExpert 0 Epoch 4, Train Loss: 6.7987\nExpert 0 Epoch 5, Train Loss: 6.6722\nExpert 0 Epoch 6, Train Loss: 6.5171\nExpert 1 Epoch 1, Train Loss: 8.5231\nExpert 1 Epoch 2, Train Loss: 8.5096\nExpert 1 Epoch 3, Train Loss: 8.3391\nExpert 1 Epoch 4, Train Loss: 9.4782\nExpert 1 Epoch 5, Train Loss: 7.9984\nExpert 1 Epoch 6, Train Loss: 9.0284\nExpert 2 Epoch 1, Train Loss: 8.4273\nExpert 2 Epoch 2, Train Loss: 8.3770\nExpert 2 Epoch 3, Train Loss: 7.6669\nExpert 2 Epoch 4, Train Loss: 8.1926\nExpert 2 Epoch 5, Train Loss: 8.0312\nExpert 2 Epoch 6, Train Loss: 8.1981\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 20:55:42,867] Trial 2 finished with value: 0.2723334232966106 and parameters: {'gating_lr': 1.3373924091049982e-05, 'gating_epochs': 9, 'expert_lr': 5.378527196272705e-05, 'expert_epochs': 6}. Best is trial 2 with value: 0.2723334232966106.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.1618\nGating Epoch 2, Train Loss: 1.0874\nGating Epoch 3, Train Loss: 1.1540\nGating Epoch 4, Train Loss: 1.0243\nGating Epoch 5, Train Loss: 1.0117\nGating Epoch 6, Train Loss: 1.1699\nExpert 0 Epoch 1, Train Loss: 8.4911\nExpert 0 Epoch 2, Train Loss: 7.6335\nExpert 0 Epoch 3, Train Loss: 7.9650\nExpert 1 Epoch 1, Train Loss: 7.8268\nExpert 1 Epoch 2, Train Loss: 8.3831\nExpert 1 Epoch 3, Train Loss: 9.1865\nExpert 2 Epoch 1, Train Loss: 8.8025\nExpert 2 Epoch 2, Train Loss: 8.8494\nExpert 2 Epoch 3, Train Loss: 8.8332\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 20:56:00,115] Trial 3 finished with value: 0.2723334232966106 and parameters: {'gating_lr': 2.0300149566087104e-05, 'gating_epochs': 6, 'expert_lr': 3.0255410006207283e-05, 'expert_epochs': 3}. Best is trial 2 with value: 0.2723334232966106.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.1567\nGating Epoch 2, Train Loss: 1.2704\nGating Epoch 3, Train Loss: 1.1117\nGating Epoch 4, Train Loss: 1.1872\nGating Epoch 5, Train Loss: 1.1143\nGating Epoch 6, Train Loss: 1.1386\nGating Epoch 7, Train Loss: 1.1457\nGating Epoch 8, Train Loss: 1.2420\nGating Epoch 9, Train Loss: 1.2019\nGating Epoch 10, Train Loss: 1.2455\nExpert 0 Epoch 1, Train Loss: 9.0788\nExpert 0 Epoch 2, Train Loss: 7.7817\nExpert 0 Epoch 3, Train Loss: 7.1346\nExpert 0 Epoch 4, Train Loss: 6.9690\nExpert 0 Epoch 5, Train Loss: 6.8768\nExpert 0 Epoch 6, Train Loss: 6.3883\nExpert 1 Epoch 1, Train Loss: 8.5003\nExpert 1 Epoch 2, Train Loss: 7.8517\nExpert 1 Epoch 3, Train Loss: 7.9393\nExpert 1 Epoch 4, Train Loss: 8.5147\nExpert 1 Epoch 5, Train Loss: 8.4649\nExpert 1 Epoch 6, Train Loss: 8.1259\nExpert 2 Epoch 1, Train Loss: 9.0304\nExpert 2 Epoch 2, Train Loss: 8.1939\nExpert 2 Epoch 3, Train Loss: 8.5570\nExpert 2 Epoch 4, Train Loss: 8.2146\nExpert 2 Epoch 5, Train Loss: 8.3347\nExpert 2 Epoch 6, Train Loss: 8.2936\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 20:56:29,842] Trial 4 finished with value: 0.2723334232966106 and parameters: {'gating_lr': 4.7122299629652036e-05, 'gating_epochs': 10, 'expert_lr': 5.4244736163590344e-05, 'expert_epochs': 6}. Best is trial 2 with value: 0.2723334232966106.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.1325\nGating Epoch 2, Train Loss: 1.1495\nGating Epoch 3, Train Loss: 1.0698\nGating Epoch 4, Train Loss: 1.2660\nGating Epoch 5, Train Loss: 1.2952\nExpert 0 Epoch 1, Train Loss: 10.0311\nExpert 0 Epoch 2, Train Loss: 5.9555\nExpert 0 Epoch 3, Train Loss: 5.8558\nExpert 0 Epoch 4, Train Loss: 5.4818\nExpert 0 Epoch 5, Train Loss: 5.3929\nExpert 0 Epoch 6, Train Loss: 5.2637\nExpert 0 Epoch 7, Train Loss: 5.1762\nExpert 0 Epoch 8, Train Loss: 5.1043\nExpert 0 Epoch 9, Train Loss: 4.9578\nExpert 1 Epoch 1, Train Loss: 8.5043\nExpert 1 Epoch 2, Train Loss: 7.8339\nExpert 1 Epoch 3, Train Loss: 8.9376\nExpert 1 Epoch 4, Train Loss: 8.7681\nExpert 1 Epoch 5, Train Loss: 8.3820\nExpert 1 Epoch 6, Train Loss: 8.7119\nExpert 1 Epoch 7, Train Loss: 8.6810\nExpert 1 Epoch 8, Train Loss: 8.6591\nExpert 1 Epoch 9, Train Loss: 9.0675\nExpert 2 Epoch 1, Train Loss: 7.8978\nExpert 2 Epoch 2, Train Loss: 7.6901\nExpert 2 Epoch 3, Train Loss: 8.4966\nExpert 2 Epoch 4, Train Loss: 8.3452\nExpert 2 Epoch 5, Train Loss: 8.8762\nExpert 2 Epoch 6, Train Loss: 8.3645\nExpert 2 Epoch 7, Train Loss: 8.9633\nExpert 2 Epoch 8, Train Loss: 8.2240\nExpert 2 Epoch 9, Train Loss: 7.4484\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 20:57:04,630] Trial 5 finished with value: -0.32766657670338944 and parameters: {'gating_lr': 0.0007685339203285668, 'gating_epochs': 5, 'expert_lr': 0.0008806969767732265, 'expert_epochs': 9}. Best is trial 2 with value: 0.2723334232966106.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.0796\nGating Epoch 2, Train Loss: 1.1052\nGating Epoch 3, Train Loss: 1.1449\nGating Epoch 4, Train Loss: 1.1308\nGating Epoch 5, Train Loss: 1.1830\nGating Epoch 6, Train Loss: 1.1104\nGating Epoch 7, Train Loss: 1.1247\nGating Epoch 8, Train Loss: 1.1197\nGating Epoch 9, Train Loss: 1.1501\nGating Epoch 10, Train Loss: 1.1547\nExpert 0 Epoch 1, Train Loss: 8.3208\nExpert 0 Epoch 2, Train Loss: 9.3641\nExpert 0 Epoch 3, Train Loss: 8.2834\nExpert 0 Epoch 4, Train Loss: 7.6718\nExpert 0 Epoch 5, Train Loss: 7.2179\nExpert 1 Epoch 1, Train Loss: 9.1766\nExpert 1 Epoch 2, Train Loss: 8.4944\nExpert 1 Epoch 3, Train Loss: 7.9644\nExpert 1 Epoch 4, Train Loss: 9.0719\nExpert 1 Epoch 5, Train Loss: 8.9040\nExpert 2 Epoch 1, Train Loss: 8.6533\nExpert 2 Epoch 2, Train Loss: 8.9811\nExpert 2 Epoch 3, Train Loss: 8.0847\nExpert 2 Epoch 4, Train Loss: 8.8539\nExpert 2 Epoch 5, Train Loss: 8.9097\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 20:57:30,843] Trial 6 finished with value: -0.32766657670338944 and parameters: {'gating_lr': 7.035190432362874e-05, 'gating_epochs': 10, 'expert_lr': 2.2483756650338913e-05, 'expert_epochs': 5}. Best is trial 2 with value: 0.2723334232966106.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.2174\nGating Epoch 2, Train Loss: 1.1029\nGating Epoch 3, Train Loss: 1.1444\nGating Epoch 4, Train Loss: 1.1685\nGating Epoch 5, Train Loss: 1.0776\nGating Epoch 6, Train Loss: 1.3123\nGating Epoch 7, Train Loss: 1.0959\nGating Epoch 8, Train Loss: 1.0957\nGating Epoch 9, Train Loss: 1.0967\nExpert 0 Epoch 1, Train Loss: 9.0240\nExpert 0 Epoch 2, Train Loss: 8.3266\nExpert 0 Epoch 3, Train Loss: 7.7788\nExpert 0 Epoch 4, Train Loss: 7.7969\nExpert 0 Epoch 5, Train Loss: 8.0650\nExpert 0 Epoch 6, Train Loss: 6.7992\nExpert 0 Epoch 7, Train Loss: 7.4439\nExpert 0 Epoch 8, Train Loss: 6.5733\nExpert 1 Epoch 1, Train Loss: 9.0224\nExpert 1 Epoch 2, Train Loss: 8.8159\nExpert 1 Epoch 3, Train Loss: 8.2515\nExpert 1 Epoch 4, Train Loss: 8.9365\nExpert 1 Epoch 5, Train Loss: 8.3065\nExpert 1 Epoch 6, Train Loss: 8.3160\nExpert 1 Epoch 7, Train Loss: 7.9756\nExpert 1 Epoch 8, Train Loss: 9.0135\nExpert 2 Epoch 1, Train Loss: 8.7916\nExpert 2 Epoch 2, Train Loss: 9.3881\nExpert 2 Epoch 3, Train Loss: 8.0553\nExpert 2 Epoch 4, Train Loss: 8.7863\nExpert 2 Epoch 5, Train Loss: 8.3505\nExpert 2 Epoch 6, Train Loss: 8.6874\nExpert 2 Epoch 7, Train Loss: 9.2629\nExpert 2 Epoch 8, Train Loss: 8.0053\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 20:58:05,865] Trial 7 finished with value: -0.32766657670338944 and parameters: {'gating_lr': 5.973444157027659e-05, 'gating_epochs': 9, 'expert_lr': 3.064725374736926e-05, 'expert_epochs': 8}. Best is trial 2 with value: 0.2723334232966106.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.2590\nGating Epoch 2, Train Loss: 1.0353\nGating Epoch 3, Train Loss: 1.2603\nGating Epoch 4, Train Loss: 1.2102\nGating Epoch 5, Train Loss: 1.1734\nGating Epoch 6, Train Loss: 1.0104\nGating Epoch 7, Train Loss: 1.2556\nGating Epoch 8, Train Loss: 1.2266\nGating Epoch 9, Train Loss: 1.2191\nGating Epoch 10, Train Loss: 1.1154\nExpert 0 Epoch 1, Train Loss: 8.3001\nExpert 0 Epoch 2, Train Loss: 6.0499\nExpert 0 Epoch 3, Train Loss: 5.7407\nExpert 0 Epoch 4, Train Loss: 5.5405\nExpert 0 Epoch 5, Train Loss: 5.3863\nExpert 0 Epoch 6, Train Loss: 5.2240\nExpert 0 Epoch 7, Train Loss: 5.1692\nExpert 1 Epoch 1, Train Loss: 8.4841\nExpert 1 Epoch 2, Train Loss: 8.0414\nExpert 1 Epoch 3, Train Loss: 8.6922\nExpert 1 Epoch 4, Train Loss: 8.2716\nExpert 1 Epoch 5, Train Loss: 9.1645\nExpert 1 Epoch 6, Train Loss: 8.8625\nExpert 1 Epoch 7, Train Loss: 8.4552\nExpert 2 Epoch 1, Train Loss: 8.0035\nExpert 2 Epoch 2, Train Loss: 8.2022\nExpert 2 Epoch 3, Train Loss: 7.9322\nExpert 2 Epoch 4, Train Loss: 8.8592\nExpert 2 Epoch 5, Train Loss: 8.7627\nExpert 2 Epoch 6, Train Loss: 8.6922\nExpert 2 Epoch 7, Train Loss: 7.5699\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 20:58:38,997] Trial 8 finished with value: -0.32766657670338944 and parameters: {'gating_lr': 1.5906958698915196e-05, 'gating_epochs': 10, 'expert_lr': 0.0009534210073054425, 'expert_epochs': 7}. Best is trial 2 with value: 0.2723334232966106.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.1080\nGating Epoch 2, Train Loss: 1.0907\nGating Epoch 3, Train Loss: 1.1378\nGating Epoch 4, Train Loss: 1.0137\nGating Epoch 5, Train Loss: 1.0308\nGating Epoch 6, Train Loss: 1.1355\nExpert 0 Epoch 1, Train Loss: 8.3170\nExpert 0 Epoch 2, Train Loss: 8.5138\nExpert 0 Epoch 3, Train Loss: 8.1230\nExpert 0 Epoch 4, Train Loss: 7.8453\nExpert 0 Epoch 5, Train Loss: 8.4285\nExpert 0 Epoch 6, Train Loss: 7.6027\nExpert 0 Epoch 7, Train Loss: 7.9420\nExpert 0 Epoch 8, Train Loss: 7.3979\nExpert 1 Epoch 1, Train Loss: 8.0707\nExpert 1 Epoch 2, Train Loss: 9.2668\nExpert 1 Epoch 3, Train Loss: 8.2195\nExpert 1 Epoch 4, Train Loss: 8.4675\nExpert 1 Epoch 5, Train Loss: 9.0194\nExpert 1 Epoch 6, Train Loss: 8.0658\nExpert 1 Epoch 7, Train Loss: 8.8575\nExpert 1 Epoch 8, Train Loss: 8.7023\nExpert 2 Epoch 1, Train Loss: 8.5094\nExpert 2 Epoch 2, Train Loss: 8.3339\nExpert 2 Epoch 3, Train Loss: 9.1252\nExpert 2 Epoch 4, Train Loss: 8.1975\nExpert 2 Epoch 5, Train Loss: 8.1516\nExpert 2 Epoch 6, Train Loss: 8.4881\nExpert 2 Epoch 7, Train Loss: 8.3149\nExpert 2 Epoch 8, Train Loss: 9.0499\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 20:59:12,842] Trial 9 finished with value: -0.32766657670338944 and parameters: {'gating_lr': 1.2369144642351554e-05, 'gating_epochs': 6, 'expert_lr': 1.2989445609013305e-05, 'expert_epochs': 8}. Best is trial 2 with value: 0.2723334232966106.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.2002\nGating Epoch 2, Train Loss: 1.1283\nGating Epoch 3, Train Loss: 1.2340\nGating Epoch 4, Train Loss: 1.1191\nGating Epoch 5, Train Loss: 1.1778\nGating Epoch 6, Train Loss: 1.0604\nGating Epoch 7, Train Loss: 1.1183\nGating Epoch 8, Train Loss: 1.0758\nExpert 0 Epoch 1, Train Loss: 8.1719\nExpert 0 Epoch 2, Train Loss: 6.3017\nExpert 0 Epoch 3, Train Loss: 6.0650\nExpert 1 Epoch 1, Train Loss: 7.7945\nExpert 1 Epoch 2, Train Loss: 8.3396\nExpert 1 Epoch 3, Train Loss: 8.3464\nExpert 2 Epoch 1, Train Loss: 8.5369\nExpert 2 Epoch 2, Train Loss: 8.8786\nExpert 2 Epoch 3, Train Loss: 9.3162\n","output_type":"stream"}],"execution_count":null}]}