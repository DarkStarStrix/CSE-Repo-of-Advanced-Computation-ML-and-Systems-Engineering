{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11424845,"sourceType":"datasetVersion","datasetId":7145865}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports\nimport gc\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom transformers import BertTokenizer, BertForSequenceClassification, T5Tokenizer, T5ForConditionalGeneration\nimport optuna\nimport os\nimport json\nfrom tqdm import tqdm\nimport numpy as np\nimport time\nimport shutil","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:44:38.408824Z","iopub.execute_input":"2025-04-15T21:44:38.409472Z","iopub.status.idle":"2025-04-15T21:45:07.103844Z","shell.execute_reply.started":"2025-04-15T21:44:38.409446Z","shell.execute_reply":"2025-04-15T21:45:07.103248Z"}},"outputs":[{"name":"stderr","text":"2025-04-15 21:44:53.707352: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744753494.002555      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744753494.091067      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Device setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nprint(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n\n# Load dataset\nwith open('/kaggle/input/moe-dataset/combined_scientific_papers.json', 'r') as f:\n    data = json.load(f)\n\n# Domain to label mapping\ndomain_to_label = {domain: idx for idx, domain in enumerate(set(entry['domain'] for entry in data))}\nnum_labels = len(domain_to_label)\nnum_experts = num_labels\nprint(f\"Number of experts/domains: {num_experts}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:45:07.104923Z","iopub.execute_input":"2025-04-15T21:45:07.105438Z","iopub.status.idle":"2025-04-15T21:45:07.135625Z","shell.execute_reply.started":"2025-04-15T21:45:07.105419Z","shell.execute_reply":"2025-04-15T21:45:07.134938Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nNumber of GPUs available: 2\nNumber of experts/domains: 3\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"class ScientificDataset(Dataset):\n    def __init__(self, data, domain_to_label):\n        self.queries = [entry['text'][:100] for entry in data]\n        self.labels = [domain_to_label[entry['domain']] for entry in data]\n        self.responses = [entry['text'] for entry in data]\n    \n    def __len__(self):\n        return len(self.queries)\n    \n    def __getitem__(self, idx):\n        return self.queries[idx], self.labels[idx], self.responses[idx]\n\n# Collate functions\ndef gating_collate_fn(batch):\n    queries, labels, _ = zip(*batch)\n    tokenized = bert_tokenizer(list(queries), padding=True, truncation=True, return_tensors='pt')\n    return tokenized, torch.tensor(labels)\n\ndef expert_collate_fn(batch):\n    queries, responses = zip(*batch)\n    inputs = t5_tokenizer(list(queries), padding=True, truncation=True, return_tensors='pt')\n    targets = t5_tokenizer(list(responses), padding=True, truncation=True, return_tensors='pt')\n    return inputs, targets['input_ids']\n\n# Expert dataset\nclass TextDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx]\n\n# Extract expert data\ndef get_expert_data(dataset, expert_id):\n    return [(query, response) for query, label, response in dataset if label == expert_id]\n\n# Training functions with gradient accumulation\ndef train_gating_model(model, train_loader, val_loader, lr, epochs, accumulation_steps=4):\n    optimizer = optim.AdamW(model.parameters(), lr=lr)\n    model = nn.DataParallel(model)\n    model.to(device)\n    loss_fct = nn.CrossEntropyLoss()\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        optimizer.zero_grad()\n        for i, batch in enumerate(train_loader):\n            inputs, labels = batch\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            labels = labels.to(device)\n            outputs = model(input_ids=inputs['input_ids'], \n                            attention_mask=inputs['attention_mask'])\n            logits = outputs.logits\n            loss = loss_fct(logits, labels) / accumulation_steps\n            loss.backward()\n            if (i + 1) % accumulation_steps == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n            total_loss += loss.item() * accumulation_steps\n        avg_loss = total_loss / len(train_loader)\n        print(f\"Gating Epoch {epoch+1}, Train Loss: {avg_loss:.4f}\")\n        gc.collect()\n        torch.cuda.empty_cache()\n    return model\n\ndef train_expert(model, train_loader, val_loader, lr, epochs, expert_id, accumulation_steps=4):\n    optimizer = optim.AdamW(model.parameters(), lr=lr)\n    model = nn.DataParallel(model)\n    model.to(device)\n    pad_token_id = model.module.config.pad_token_id\n    loss_fct = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        optimizer.zero_grad()\n        for i, batch in enumerate(train_loader):\n            inputs, labels = batch\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            labels = labels.to(device)\n            decoder_input_ids = labels[:, :-1].clone()\n            target_labels = labels[:, 1:].clone()\n            outputs = model(input_ids=inputs['input_ids'], \n                            attention_mask=inputs['attention_mask'], \n                            decoder_input_ids=decoder_input_ids)\n            logits = outputs.logits\n            loss = loss_fct(logits.view(-1, logits.size(-1)), target_labels.view(-1)) / accumulation_steps\n            loss.backward()\n            if (i + 1) % accumulation_steps == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n            total_loss += loss.item() * accumulation_steps\n        avg_loss = total_loss / len(train_loader)\n        print(f\"Expert {expert_id} Epoch {epoch+1}, Train Loss: {avg_loss:.4f}\")\n        gc.collect()\n        torch.cuda.empty_cache()\n    return model\n\n# Evaluation functions\ndef evaluate_gating_model(model, val_loader):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            inputs, labels = batch\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            labels = labels.to(device)\n            outputs = model(input_ids=inputs['input_ids'], \n                           attention_mask=inputs['attention_mask'])\n            _, predicted = torch.max(outputs.logits, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return correct / total\n\ndef evaluate_expert(model, val_loader):\n    if len(val_loader) == 0:\n        return 0.0\n    model.eval()\n    total_loss = 0\n    pad_token_id = model.module.config.pad_token_id\n    loss_fct = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n    with torch.no_grad():\n        for batch in val_loader:\n            inputs, labels = batch\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            labels = labels.to(device)\n            decoder_input_ids = labels[:, :-1].clone()\n            target_labels = labels[:, 1:].clone()\n            outputs = model(input_ids=inputs['input_ids'], \n                            attention_mask=inputs['attention_mask'], \n                            decoder_input_ids=decoder_input_ids)\n            logits = outputs.logits\n            loss = loss_fct(logits.view(-1, logits.size(-1)), target_labels.view(-1))\n            total_loss += loss.item()\n    return total_loss / len(val_loader)\n\n# Optuna objective\ndef objective(trial):\n    trial_dir = f'trial_{trial.number}'\n    os.makedirs(trial_dir, exist_ok=True)\n    \n    gating_lr = trial.suggest_float('gating_lr', 1e-5, 1e-3, log=True)\n    gating_epochs = trial.suggest_int('gating_epochs', 3, 10)\n    expert_lr = trial.suggest_float('expert_lr', 1e-5, 1e-3, log=True)\n    expert_epochs = trial.suggest_int('expert_epochs', 3, 10)\n    \n    gating_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_experts).to(device)\n    gating_model = train_gating_model(gating_model, train_loader_gating, val_loader_gating, gating_lr, gating_epochs)\n    gating_accuracy = evaluate_gating_model(gating_model, val_loader_gating)\n    \n    gating_model_path = os.path.join(trial_dir, 'gating.pt')\n    torch.save(gating_model.state_dict(), gating_model_path)\n    gating_model.to('cpu')\n    torch.cuda.empty_cache()\n    del gating_model\n    \n    experts_paths = {}\n    expert_losses = []\n    for expert_id in range(num_experts):\n        train_data = train_expert_data[expert_id]\n        val_data = val_expert_data[expert_id]\n        if not train_data:\n            continue\n        expert_model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n        train_dataset_expert = TextDataset(train_data)\n        val_dataset_expert = TextDataset(val_data)\n        train_loader_expert = DataLoader(train_dataset_expert, batch_size=2, shuffle=True, collate_fn=expert_collate_fn)\n        val_loader_expert = DataLoader(val_dataset_expert, batch_size=2, shuffle=False, collate_fn=expert_collate_fn)\n        expert_model = train_expert(expert_model, train_loader_expert, val_loader_expert, expert_lr, expert_epochs, expert_id)\n        loss = evaluate_expert(expert_model, val_loader_expert)\n        expert_losses.append(loss)\n        \n        expert_path = os.path.join(trial_dir, f'expert_{expert_id}.pt')\n        torch.save(expert_model.state_dict(), expert_path)\n        experts_paths[expert_id] = expert_path\n        expert_model.to('cpu')\n        torch.cuda.empty_cache()\n        del expert_model\n    \n    avg_expert_loss = sum(expert_losses) / len(expert_losses) if expert_losses else 0\n    combined_metric = gating_accuracy - 0.1 * avg_expert_loss\n    \n    trial.set_user_attr('gating_accuracy', gating_accuracy)\n    trial.set_user_attr('avg_expert_loss', avg_expert_loss)\n    trial.set_user_attr('trial_dir', trial_dir)\n    trial.set_user_attr('experts_paths', experts_paths)\n    \n    return combined_metric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:45:07.136388Z","iopub.execute_input":"2025-04-15T21:45:07.136660Z","iopub.status.idle":"2025-04-15T21:45:07.160027Z","shell.execute_reply.started":"2025-04-15T21:45:07.136636Z","shell.execute_reply":"2025-04-15T21:45:07.159239Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    full_dataset = ScientificDataset(data, domain_to_label)\n    train_size = int(0.8 * len(full_dataset))\n    val_size = len(full_dataset) - train_size\n    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n    \n    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    t5_tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False)\n    \n    train_loader_gating = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=gating_collate_fn)\n    val_loader_gating = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=gating_collate_fn)\n    \n    train_expert_data = {i: get_expert_data(train_dataset, i) for i in range(num_experts)}\n    val_expert_data = {i: get_expert_data(val_dataset, i) for i in range(num_experts)}\n    \n    study = optuna.create_study(direction='maximize')\n    study.optimize(objective, n_trials=20)\n    \n    print(\"\\nBenchmarking Top 5 Configurations:\")\n    top_trials = sorted(study.trials, key=lambda t: t.value, reverse=True)[:5]\n    for i, trial in enumerate(top_trials):\n        print(f\"\\nMOE {i+1}:\")\n        print(f\"  Trial Number: {trial.number}\")\n        print(f\"  Combined Metric: {trial.value:.4f}\")\n        print(f\"  Gating Accuracy: {trial.user_attrs['gating_accuracy']:.4f}\")\n        print(f\"  Avg Expert Loss: {trial.user_attrs['avg_expert_loss']:.4f}\")\n        print(f\"  Hyperparameters: {trial.params}\")\n    \n    for i, trial in enumerate(top_trials):\n        moe_dir = f'MOE_{i+1}'\n        os.makedirs(moe_dir, exist_ok=True)\n        trial_dir = trial.user_attrs['trial_dir']\n        \n        shutil.copy(os.path.join(trial_dir, 'gating.pt'), os.path.join(moe_dir, 'gating.pt'))\n        \n        experts_paths = trial.user_attrs['experts_paths']\n        for expert_id, expert_path in experts_paths.items():\n            shutil.copy(expert_path, os.path.join(moe_dir, f'expert_{expert_id}.pt'))\n        \n        metrics = {\n            'gating_accuracy': trial.user_attrs['gating_accuracy'],\n            'avg_expert_loss': trial.user_attrs['avg_expert_loss'],\n            'combined_metric': trial.value\n        }\n        with open(os.path.join(moe_dir, 'metrics.json'), 'w') as f:\n            json.dump(metrics, f)\n        with open(os.path.join(moe_dir, 'hyperparams.json'), 'w') as f:\n            json.dump(trial.params, f)\n    \n    top_trial_numbers = [trial.number for trial in top_trials]\n    for trial in study.trials:\n        if trial.number not in top_trial_numbers:\n            trial_dir = trial.user_attrs.get('trial_dir')\n            if trial_dir and os.path.exists(trial_dir):\n                shutil.rmtree(trial_dir)\n    \n    print(\"\\nTop 5 MOE models saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:45:07.161533Z","iopub.execute_input":"2025-04-15T21:45:07.161824Z","iopub.status.idle":"2025-04-15T21:54:53.047076Z","shell.execute_reply.started":"2025-04-15T21:45:07.161791Z","shell.execute_reply":"2025-04-15T21:54:53.045934Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11b73d87c8af4311ad7265a97cc31841"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1791ffb0bf341328c35eaf546c3de69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cd7827bc9b640578d1a132c596c290e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2edd1b9dfd92498eb49aa3b4d64af089"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec7d61facd92475e8dd967f5335ffb15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03b484c14437415a89202018e40b60e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c6346c860c9464abe02551a7cc50e67"}},"metadata":{}},{"name":"stderr","text":"[I 2025-04-15 21:45:08,933] A new study created in memory with name: no-name-24f4c29d-9c17-4f33-8228-49fd2ec7feed\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b3658ddcaa94320bbf24ed47649ae9c"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.2060\nGating Epoch 2, Train Loss: 1.2355\nGating Epoch 3, Train Loss: 1.1713\nGating Epoch 4, Train Loss: 0.9235\nGating Epoch 5, Train Loss: 1.1551\nGating Epoch 6, Train Loss: 0.9173\nGating Epoch 7, Train Loss: 0.9830\nGating Epoch 8, Train Loss: 0.9875\nGating Epoch 9, Train Loss: 1.1635\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"011257737b284ae7b4541e58cadb2001"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f84b661c28be4e7b8278b617e4d97891"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7db24f5da824a3495554969a644b93f"}},"metadata":{}},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"name":"stdout","text":"Expert 0 Epoch 1, Train Loss: 8.2312\nExpert 0 Epoch 2, Train Loss: 7.7561\nExpert 0 Epoch 3, Train Loss: 7.4886\nExpert 0 Epoch 4, Train Loss: 7.8170\nExpert 0 Epoch 5, Train Loss: 8.2777\nExpert 0 Epoch 6, Train Loss: 9.2387\nExpert 0 Epoch 7, Train Loss: 8.2158\nExpert 0 Epoch 8, Train Loss: 8.2186\nExpert 0 Epoch 9, Train Loss: 8.4419\nExpert 0 Epoch 10, Train Loss: 8.0054\nExpert 1 Epoch 1, Train Loss: 9.1898\nExpert 1 Epoch 2, Train Loss: 5.7921\nExpert 1 Epoch 3, Train Loss: 5.7660\nExpert 1 Epoch 4, Train Loss: 5.4415\nExpert 1 Epoch 5, Train Loss: 5.3396\nExpert 1 Epoch 6, Train Loss: 5.3139\nExpert 1 Epoch 7, Train Loss: 5.2141\nExpert 1 Epoch 8, Train Loss: 5.1516\nExpert 1 Epoch 9, Train Loss: 5.0151\nExpert 1 Epoch 10, Train Loss: 5.0468\nExpert 2 Epoch 1, Train Loss: 9.2142\nExpert 2 Epoch 2, Train Loss: 8.5644\nExpert 2 Epoch 3, Train Loss: 8.8171\nExpert 2 Epoch 4, Train Loss: 8.7581\nExpert 2 Epoch 5, Train Loss: 9.2824\nExpert 2 Epoch 6, Train Loss: 8.2846\nExpert 2 Epoch 7, Train Loss: 8.6929\nExpert 2 Epoch 8, Train Loss: 8.8988\nExpert 2 Epoch 9, Train Loss: 8.9636\nExpert 2 Epoch 10, Train Loss: 9.0418\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 21:45:56,393] Trial 0 finished with value: -0.2897994836171468 and parameters: {'gating_lr': 0.0009911161304626812, 'gating_epochs': 9, 'expert_lr': 0.0005427763810654496, 'expert_epochs': 10}. Best is trial 0 with value: -0.2897994836171468.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.1303\nGating Epoch 2, Train Loss: 1.0080\nGating Epoch 3, Train Loss: 1.0009\nGating Epoch 4, Train Loss: 1.1083\nExpert 0 Epoch 1, Train Loss: 7.6656\nExpert 0 Epoch 2, Train Loss: 8.3227\nExpert 0 Epoch 3, Train Loss: 7.6131\nExpert 0 Epoch 4, Train Loss: 7.7927\nExpert 1 Epoch 1, Train Loss: 9.1275\nExpert 1 Epoch 2, Train Loss: 8.3206\nExpert 1 Epoch 3, Train Loss: 7.7673\nExpert 1 Epoch 4, Train Loss: 7.2721\nExpert 2 Epoch 1, Train Loss: 8.7710\nExpert 2 Epoch 2, Train Loss: 8.7408\nExpert 2 Epoch 3, Train Loss: 9.0971\nExpert 2 Epoch 4, Train Loss: 8.6869\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 21:46:15,400] Trial 1 finished with value: -0.31526645024617517 and parameters: {'gating_lr': 0.000506561536060624, 'gating_epochs': 4, 'expert_lr': 3.393877654448231e-05, 'expert_epochs': 4}. Best is trial 0 with value: -0.2897994836171468.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.2853\nGating Epoch 2, Train Loss: 1.2600\nGating Epoch 3, Train Loss: 1.1154\nGating Epoch 4, Train Loss: 1.3207\nGating Epoch 5, Train Loss: 1.2868\nGating Epoch 6, Train Loss: 1.3152\nGating Epoch 7, Train Loss: 1.1370\nExpert 0 Epoch 1, Train Loss: 8.8826\nExpert 0 Epoch 2, Train Loss: 7.7605\nExpert 0 Epoch 3, Train Loss: 8.3009\nExpert 0 Epoch 4, Train Loss: 7.9384\nExpert 0 Epoch 5, Train Loss: 7.7097\nExpert 0 Epoch 6, Train Loss: 8.2612\nExpert 0 Epoch 7, Train Loss: 8.8336\nExpert 0 Epoch 8, Train Loss: 7.6416\nExpert 0 Epoch 9, Train Loss: 7.9992\nExpert 1 Epoch 1, Train Loss: 10.2315\nExpert 1 Epoch 2, Train Loss: 6.1280\nExpert 1 Epoch 3, Train Loss: 5.9215\nExpert 1 Epoch 4, Train Loss: 5.7991\nExpert 1 Epoch 5, Train Loss: 5.6710\nExpert 1 Epoch 6, Train Loss: 5.6090\nExpert 1 Epoch 7, Train Loss: 5.4465\nExpert 1 Epoch 8, Train Loss: 5.3993\nExpert 1 Epoch 9, Train Loss: 5.2974\nExpert 2 Epoch 1, Train Loss: 8.2756\nExpert 2 Epoch 2, Train Loss: 10.2850\nExpert 2 Epoch 3, Train Loss: 9.0873\nExpert 2 Epoch 4, Train Loss: 9.3388\nExpert 2 Epoch 5, Train Loss: 9.2437\nExpert 2 Epoch 6, Train Loss: 8.8488\nExpert 2 Epoch 7, Train Loss: 8.7962\nExpert 2 Epoch 8, Train Loss: 8.3770\nExpert 2 Epoch 9, Train Loss: 8.1298\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 21:46:52,077] Trial 2 finished with value: -0.29424751599629717 and parameters: {'gating_lr': 0.0003180065817088647, 'gating_epochs': 7, 'expert_lr': 0.0002766300816983877, 'expert_epochs': 9}. Best is trial 0 with value: -0.2897994836171468.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.0681\nGating Epoch 2, Train Loss: 1.0708\nGating Epoch 3, Train Loss: 1.1274\nGating Epoch 4, Train Loss: 1.1591\nGating Epoch 5, Train Loss: 1.0243\nGating Epoch 6, Train Loss: 1.1033\nGating Epoch 7, Train Loss: 1.1006\nExpert 0 Epoch 1, Train Loss: 8.3030\nExpert 0 Epoch 2, Train Loss: 8.0816\nExpert 0 Epoch 3, Train Loss: 8.5177\nExpert 0 Epoch 4, Train Loss: 8.2028\nExpert 1 Epoch 1, Train Loss: 9.5439\nExpert 1 Epoch 2, Train Loss: 10.0005\nExpert 1 Epoch 3, Train Loss: 7.8773\nExpert 1 Epoch 4, Train Loss: 8.0169\nExpert 2 Epoch 1, Train Loss: 8.9574\nExpert 2 Epoch 2, Train Loss: 9.2676\nExpert 2 Epoch 3, Train Loss: 8.7881\nExpert 2 Epoch 4, Train Loss: 9.4242\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 21:47:13,430] Trial 3 finished with value: -0.5206979910532634 and parameters: {'gating_lr': 1.654524998237207e-05, 'gating_epochs': 7, 'expert_lr': 1.856809070715814e-05, 'expert_epochs': 4}. Best is trial 0 with value: -0.2897994836171468.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.0548\nGating Epoch 2, Train Loss: 1.1568\nGating Epoch 3, Train Loss: 1.0073\nGating Epoch 4, Train Loss: 1.1348\nExpert 0 Epoch 1, Train Loss: 8.5807\nExpert 0 Epoch 2, Train Loss: 7.6978\nExpert 0 Epoch 3, Train Loss: 8.1332\nExpert 0 Epoch 4, Train Loss: 8.2297\nExpert 0 Epoch 5, Train Loss: 9.4223\nExpert 0 Epoch 6, Train Loss: 7.8461\nExpert 0 Epoch 7, Train Loss: 7.5361\nExpert 1 Epoch 1, Train Loss: 8.8145\nExpert 1 Epoch 2, Train Loss: 8.6456\nExpert 1 Epoch 3, Train Loss: 7.6634\nExpert 1 Epoch 4, Train Loss: 7.1211\nExpert 1 Epoch 5, Train Loss: 6.8866\nExpert 1 Epoch 6, Train Loss: 6.4203\nExpert 1 Epoch 7, Train Loss: 6.2186\nExpert 2 Epoch 1, Train Loss: 9.7169\nExpert 2 Epoch 2, Train Loss: 8.9009\nExpert 2 Epoch 3, Train Loss: 8.6333\nExpert 2 Epoch 4, Train Loss: 8.9676\nExpert 2 Epoch 5, Train Loss: 9.1477\nExpert 2 Epoch 6, Train Loss: 8.4958\nExpert 2 Epoch 7, Train Loss: 9.0224\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 21:47:41,889] Trial 4 finished with value: -0.5084701379140217 and parameters: {'gating_lr': 1.027185267388475e-05, 'gating_epochs': 4, 'expert_lr': 4.2237230602717784e-05, 'expert_epochs': 7}. Best is trial 0 with value: -0.2897994836171468.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.2152\nGating Epoch 2, Train Loss: 1.1378\nGating Epoch 3, Train Loss: 1.1765\nGating Epoch 4, Train Loss: 1.1772\nGating Epoch 5, Train Loss: 1.1087\nGating Epoch 6, Train Loss: 1.0621\nGating Epoch 7, Train Loss: 1.1538\nGating Epoch 8, Train Loss: 1.1559\nGating Epoch 9, Train Loss: 1.2132\nGating Epoch 10, Train Loss: 1.1737\nExpert 0 Epoch 1, Train Loss: 7.6325\nExpert 0 Epoch 2, Train Loss: 8.2443\nExpert 0 Epoch 3, Train Loss: 7.2584\nExpert 0 Epoch 4, Train Loss: 7.6072\nExpert 0 Epoch 5, Train Loss: 8.4710\nExpert 0 Epoch 6, Train Loss: 8.8078\nExpert 0 Epoch 7, Train Loss: 7.5961\nExpert 0 Epoch 8, Train Loss: 8.7542\nExpert 0 Epoch 9, Train Loss: 7.6933\nExpert 0 Epoch 10, Train Loss: 8.5945\nExpert 1 Epoch 1, Train Loss: 8.7822\nExpert 1 Epoch 2, Train Loss: 6.0136\nExpert 1 Epoch 3, Train Loss: 5.6983\nExpert 1 Epoch 4, Train Loss: 5.4442\nExpert 1 Epoch 5, Train Loss: 5.4564\nExpert 1 Epoch 6, Train Loss: 5.2446\nExpert 1 Epoch 7, Train Loss: 5.2197\nExpert 1 Epoch 8, Train Loss: 5.1328\nExpert 1 Epoch 9, Train Loss: 5.0940\nExpert 1 Epoch 10, Train Loss: 4.9788\nExpert 2 Epoch 1, Train Loss: 8.8489\nExpert 2 Epoch 2, Train Loss: 9.2180\nExpert 2 Epoch 3, Train Loss: 8.7981\nExpert 2 Epoch 4, Train Loss: 9.1177\nExpert 2 Epoch 5, Train Loss: 8.5543\nExpert 2 Epoch 6, Train Loss: 8.5571\nExpert 2 Epoch 7, Train Loss: 8.6992\nExpert 2 Epoch 8, Train Loss: 8.7340\nExpert 2 Epoch 9, Train Loss: 8.9041\nExpert 2 Epoch 10, Train Loss: 8.7744\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 21:48:23,632] Trial 5 finished with value: -0.28928737640380864 and parameters: {'gating_lr': 0.00040774988128485385, 'gating_epochs': 10, 'expert_lr': 0.0008169895028253811, 'expert_epochs': 10}. Best is trial 5 with value: -0.28928737640380864.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.1421\nGating Epoch 2, Train Loss: 1.0873\nGating Epoch 3, Train Loss: 1.1492\nGating Epoch 4, Train Loss: 1.1058\nExpert 0 Epoch 1, Train Loss: 7.6777\nExpert 0 Epoch 2, Train Loss: 8.3658\nExpert 0 Epoch 3, Train Loss: 8.1979\nExpert 0 Epoch 4, Train Loss: 8.0754\nExpert 0 Epoch 5, Train Loss: 7.6848\nExpert 0 Epoch 6, Train Loss: 8.2257\nExpert 0 Epoch 7, Train Loss: 8.0889\nExpert 0 Epoch 8, Train Loss: 8.8126\nExpert 0 Epoch 9, Train Loss: 8.5274\nExpert 0 Epoch 10, Train Loss: 7.6637\nExpert 1 Epoch 1, Train Loss: 8.7390\nExpert 1 Epoch 2, Train Loss: 5.9587\nExpert 1 Epoch 3, Train Loss: 5.6661\nExpert 1 Epoch 4, Train Loss: 5.5740\nExpert 1 Epoch 5, Train Loss: 5.4383\nExpert 1 Epoch 6, Train Loss: 5.2106\nExpert 1 Epoch 7, Train Loss: 5.1748\nExpert 1 Epoch 8, Train Loss: 5.1967\nExpert 1 Epoch 9, Train Loss: 5.0256\nExpert 1 Epoch 10, Train Loss: 5.0589\nExpert 2 Epoch 1, Train Loss: 8.4180\nExpert 2 Epoch 2, Train Loss: 8.8140\nExpert 2 Epoch 3, Train Loss: 9.3249\nExpert 2 Epoch 4, Train Loss: 8.9511\nExpert 2 Epoch 5, Train Loss: 8.7703\nExpert 2 Epoch 6, Train Loss: 9.2318\nExpert 2 Epoch 7, Train Loss: 9.4216\nExpert 2 Epoch 8, Train Loss: 9.1250\nExpert 2 Epoch 9, Train Loss: 8.8631\nExpert 2 Epoch 10, Train Loss: 9.4042\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 21:49:01,203] Trial 6 finished with value: -0.48967701594034835 and parameters: {'gating_lr': 0.0003836214473827223, 'gating_epochs': 4, 'expert_lr': 0.00043798355049078384, 'expert_epochs': 10}. Best is trial 5 with value: -0.28928737640380864.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.2126\nGating Epoch 2, Train Loss: 1.0999\nGating Epoch 3, Train Loss: 1.0643\nGating Epoch 4, Train Loss: 1.1759\nGating Epoch 5, Train Loss: 1.1019\nGating Epoch 6, Train Loss: 1.0282\nGating Epoch 7, Train Loss: 1.0804\nExpert 0 Epoch 1, Train Loss: 7.8414\nExpert 0 Epoch 2, Train Loss: 8.3951\nExpert 0 Epoch 3, Train Loss: 7.6248\nExpert 1 Epoch 1, Train Loss: 8.9294\nExpert 1 Epoch 2, Train Loss: 8.0333\nExpert 1 Epoch 3, Train Loss: 7.2104\nExpert 2 Epoch 1, Train Loss: 9.0886\nExpert 2 Epoch 2, Train Loss: 9.0734\nExpert 2 Epoch 3, Train Loss: 9.2205\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 21:49:19,724] Trial 7 finished with value: -0.3131669203440349 and parameters: {'gating_lr': 4.296749905535592e-05, 'gating_epochs': 7, 'expert_lr': 6.058681485871738e-05, 'expert_epochs': 3}. Best is trial 5 with value: -0.28928737640380864.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 0.9881\nGating Epoch 2, Train Loss: 1.0262\nGating Epoch 3, Train Loss: 1.0384\nExpert 0 Epoch 1, Train Loss: 7.5086\nExpert 0 Epoch 2, Train Loss: 7.1770\nExpert 0 Epoch 3, Train Loss: 8.8499\nExpert 0 Epoch 4, Train Loss: 8.4360\nExpert 0 Epoch 5, Train Loss: 8.2254\nExpert 1 Epoch 1, Train Loss: 8.9629\nExpert 1 Epoch 2, Train Loss: 8.7145\nExpert 1 Epoch 3, Train Loss: 8.4195\nExpert 1 Epoch 4, Train Loss: 8.1604\nExpert 1 Epoch 5, Train Loss: 8.8194\nExpert 2 Epoch 1, Train Loss: 9.1794\nExpert 2 Epoch 2, Train Loss: 8.7201\nExpert 2 Epoch 3, Train Loss: 8.5351\nExpert 2 Epoch 4, Train Loss: 8.3182\nExpert 2 Epoch 5, Train Loss: 8.4079\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 21:49:41,611] Trial 8 finished with value: -0.3215205669403076 and parameters: {'gating_lr': 3.716219741163235e-05, 'gating_epochs': 3, 'expert_lr': 1.3429533478405216e-05, 'expert_epochs': 5}. Best is trial 5 with value: -0.28928737640380864.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.2183\nGating Epoch 2, Train Loss: 1.0117\nGating Epoch 3, Train Loss: 1.1764\nGating Epoch 4, Train Loss: 1.0414\nGating Epoch 5, Train Loss: 1.0853\nGating Epoch 6, Train Loss: 1.1459\nExpert 0 Epoch 1, Train Loss: 7.9300\nExpert 0 Epoch 2, Train Loss: 7.5311\nExpert 0 Epoch 3, Train Loss: 7.1269\nExpert 0 Epoch 4, Train Loss: 8.1766\nExpert 1 Epoch 1, Train Loss: 8.9009\nExpert 1 Epoch 2, Train Loss: 9.0852\nExpert 1 Epoch 3, Train Loss: 8.5183\nExpert 1 Epoch 4, Train Loss: 7.8121\nExpert 2 Epoch 1, Train Loss: 9.6349\nExpert 2 Epoch 2, Train Loss: 9.4274\nExpert 2 Epoch 3, Train Loss: 8.5067\nExpert 2 Epoch 4, Train Loss: 8.9772\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 21:50:02,214] Trial 9 finished with value: -0.5259895006815594 and parameters: {'gating_lr': 0.0003021363581110624, 'gating_epochs': 6, 'expert_lr': 1.2429756702914153e-05, 'expert_epochs': 4}. Best is trial 5 with value: -0.28928737640380864.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.1132\nGating Epoch 2, Train Loss: 1.1241\nGating Epoch 3, Train Loss: 1.0881\nGating Epoch 4, Train Loss: 1.1610\nGating Epoch 5, Train Loss: 1.0685\nGating Epoch 6, Train Loss: 1.0695\nGating Epoch 7, Train Loss: 1.0169\nGating Epoch 8, Train Loss: 1.0784\nGating Epoch 9, Train Loss: 1.1131\nGating Epoch 10, Train Loss: 1.1049\nExpert 0 Epoch 1, Train Loss: 7.6051\nExpert 0 Epoch 2, Train Loss: 8.9445\nExpert 0 Epoch 3, Train Loss: 8.4976\nExpert 0 Epoch 4, Train Loss: 8.3767\nExpert 0 Epoch 5, Train Loss: 8.2155\nExpert 0 Epoch 6, Train Loss: 7.9460\nExpert 0 Epoch 7, Train Loss: 8.1620\nExpert 0 Epoch 8, Train Loss: 8.1974\nExpert 1 Epoch 1, Train Loss: 9.4135\nExpert 1 Epoch 2, Train Loss: 5.7588\nExpert 1 Epoch 3, Train Loss: 5.6523\nExpert 1 Epoch 4, Train Loss: 5.5078\nExpert 1 Epoch 5, Train Loss: 5.3447\nExpert 1 Epoch 6, Train Loss: 5.2133\nExpert 1 Epoch 7, Train Loss: 5.0942\nExpert 1 Epoch 8, Train Loss: 5.0731\nExpert 2 Epoch 1, Train Loss: 8.5985\nExpert 2 Epoch 2, Train Loss: 8.9545\nExpert 2 Epoch 3, Train Loss: 9.6734\nExpert 2 Epoch 4, Train Loss: 8.9266\nExpert 2 Epoch 5, Train Loss: 8.5841\nExpert 2 Epoch 6, Train Loss: 8.2316\nExpert 2 Epoch 7, Train Loss: 8.4039\nExpert 2 Epoch 8, Train Loss: 9.1541\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 21:50:37,930] Trial 10 finished with value: -0.2947918891906739 and parameters: {'gating_lr': 9.795984896871263e-05, 'gating_epochs': 10, 'expert_lr': 0.0009987505771563818, 'expert_epochs': 8}. Best is trial 5 with value: -0.28928737640380864.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.3967\nGating Epoch 2, Train Loss: 1.2883\nGating Epoch 3, Train Loss: 1.3922\nGating Epoch 4, Train Loss: 0.9851\nGating Epoch 5, Train Loss: 1.0155\nGating Epoch 6, Train Loss: 1.0420\nGating Epoch 7, Train Loss: 1.2935\nGating Epoch 8, Train Loss: 1.3461\nGating Epoch 9, Train Loss: 1.0469\nGating Epoch 10, Train Loss: 1.1898\nExpert 0 Epoch 1, Train Loss: 7.7791\nExpert 0 Epoch 2, Train Loss: 8.4497\nExpert 0 Epoch 3, Train Loss: 8.6435\nExpert 0 Epoch 4, Train Loss: 7.9820\nExpert 0 Epoch 5, Train Loss: 8.1726\nExpert 0 Epoch 6, Train Loss: 7.8955\nExpert 0 Epoch 7, Train Loss: 8.1099\nExpert 0 Epoch 8, Train Loss: 7.9911\nExpert 0 Epoch 9, Train Loss: 7.8059\nExpert 0 Epoch 10, Train Loss: 7.5531\nExpert 1 Epoch 1, Train Loss: 9.0302\nExpert 1 Epoch 2, Train Loss: 6.7024\nExpert 1 Epoch 3, Train Loss: 6.4476\nExpert 1 Epoch 4, Train Loss: 6.0891\nExpert 1 Epoch 5, Train Loss: 5.9259\nExpert 1 Epoch 6, Train Loss: 5.7702\nExpert 1 Epoch 7, Train Loss: 5.7009\nExpert 1 Epoch 8, Train Loss: 5.5432\nExpert 1 Epoch 9, Train Loss: 5.5756\nExpert 1 Epoch 10, Train Loss: 5.2902\nExpert 2 Epoch 1, Train Loss: 8.8575\nExpert 2 Epoch 2, Train Loss: 8.5833\nExpert 2 Epoch 3, Train Loss: 8.4466\nExpert 2 Epoch 4, Train Loss: 9.1086\nExpert 2 Epoch 5, Train Loss: 8.4567\nExpert 2 Epoch 6, Train Loss: 9.0553\nExpert 2 Epoch 7, Train Loss: 9.5846\nExpert 2 Epoch 8, Train Loss: 8.9296\nExpert 2 Epoch 9, Train Loss: 8.9671\nExpert 2 Epoch 10, Train Loss: 8.9485\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 21:51:19,459] Trial 11 finished with value: -0.4963113625844319 and parameters: {'gating_lr': 0.000994050459357635, 'gating_epochs': 10, 'expert_lr': 0.0001989565542994661, 'expert_epochs': 10}. Best is trial 5 with value: -0.28928737640380864.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.2488\nGating Epoch 2, Train Loss: 1.2804\nGating Epoch 3, Train Loss: 1.0100\nGating Epoch 4, Train Loss: 0.9873\nGating Epoch 5, Train Loss: 1.0504\nGating Epoch 6, Train Loss: 1.1079\nGating Epoch 7, Train Loss: 1.0846\nGating Epoch 8, Train Loss: 0.9970\nGating Epoch 9, Train Loss: 1.1526\nExpert 0 Epoch 1, Train Loss: 7.8261\nExpert 0 Epoch 2, Train Loss: 8.5040\nExpert 0 Epoch 3, Train Loss: 7.6919\nExpert 0 Epoch 4, Train Loss: 7.6483\nExpert 0 Epoch 5, Train Loss: 8.2610\nExpert 0 Epoch 6, Train Loss: 8.3433\nExpert 0 Epoch 7, Train Loss: 7.5100\nExpert 0 Epoch 8, Train Loss: 7.7797\nExpert 1 Epoch 1, Train Loss: 8.4910\nExpert 1 Epoch 2, Train Loss: 5.9717\nExpert 1 Epoch 3, Train Loss: 5.5762\nExpert 1 Epoch 4, Train Loss: 5.4418\nExpert 1 Epoch 5, Train Loss: 5.3413\nExpert 1 Epoch 6, Train Loss: 5.2629\nExpert 1 Epoch 7, Train Loss: 5.1099\nExpert 1 Epoch 8, Train Loss: 5.0861\nExpert 2 Epoch 1, Train Loss: 8.8225\nExpert 2 Epoch 2, Train Loss: 8.7558\nExpert 2 Epoch 3, Train Loss: 9.2576\nExpert 2 Epoch 4, Train Loss: 8.6535\nExpert 2 Epoch 5, Train Loss: 8.6325\nExpert 2 Epoch 6, Train Loss: 8.4274\nExpert 2 Epoch 7, Train Loss: 9.2253\nExpert 2 Epoch 8, Train Loss: 8.4999\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 21:51:54,237] Trial 12 finished with value: -0.4905422369639079 and parameters: {'gating_lr': 0.0009833292875574562, 'gating_epochs': 9, 'expert_lr': 0.0006783403396056237, 'expert_epochs': 8}. Best is trial 5 with value: -0.28928737640380864.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.0576\nGating Epoch 2, Train Loss: 1.1032\nGating Epoch 3, Train Loss: 1.1949\nGating Epoch 4, Train Loss: 1.0198\nGating Epoch 5, Train Loss: 1.0426\nGating Epoch 6, Train Loss: 1.1575\nGating Epoch 7, Train Loss: 1.0976\nGating Epoch 8, Train Loss: 1.0301\nGating Epoch 9, Train Loss: 1.1381\nExpert 0 Epoch 1, Train Loss: 8.5423\nExpert 0 Epoch 2, Train Loss: 8.4561\nExpert 0 Epoch 3, Train Loss: 8.1778\nExpert 0 Epoch 4, Train Loss: 8.8518\nExpert 0 Epoch 5, Train Loss: 8.2354\nExpert 0 Epoch 6, Train Loss: 8.0333\nExpert 0 Epoch 7, Train Loss: 8.7001\nExpert 0 Epoch 8, Train Loss: 8.1543\nExpert 0 Epoch 9, Train Loss: 8.3933\nExpert 0 Epoch 10, Train Loss: 8.3865\nExpert 1 Epoch 1, Train Loss: 8.2817\nExpert 1 Epoch 2, Train Loss: 6.6001\nExpert 1 Epoch 3, Train Loss: 6.2879\nExpert 1 Epoch 4, Train Loss: 6.0260\nExpert 1 Epoch 5, Train Loss: 5.9625\nExpert 1 Epoch 6, Train Loss: 5.6018\nExpert 1 Epoch 7, Train Loss: 5.6865\nExpert 1 Epoch 8, Train Loss: 5.5057\nExpert 1 Epoch 9, Train Loss: 5.3896\nExpert 1 Epoch 10, Train Loss: 5.4241\nExpert 2 Epoch 1, Train Loss: 8.7469\nExpert 2 Epoch 2, Train Loss: 9.5710\nExpert 2 Epoch 3, Train Loss: 9.4256\nExpert 2 Epoch 4, Train Loss: 9.1368\nExpert 2 Epoch 5, Train Loss: 9.5361\nExpert 2 Epoch 6, Train Loss: 7.8507\nExpert 2 Epoch 7, Train Loss: 9.2432\nExpert 2 Epoch 8, Train Loss: 9.3323\nExpert 2 Epoch 9, Train Loss: 9.0842\nExpert 2 Epoch 10, Train Loss: 8.8477\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 21:52:35,114] Trial 13 finished with value: -0.49651103019714354 and parameters: {'gating_lr': 0.0001741872896918878, 'gating_epochs': 9, 'expert_lr': 0.0001629267280785093, 'expert_epochs': 10}. Best is trial 5 with value: -0.28928737640380864.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.0607\nGating Epoch 2, Train Loss: 1.0815\nGating Epoch 3, Train Loss: 1.1205\nGating Epoch 4, Train Loss: 1.1136\nGating Epoch 5, Train Loss: 1.0972\nGating Epoch 6, Train Loss: 1.0898\nGating Epoch 7, Train Loss: 1.1257\nGating Epoch 8, Train Loss: 1.0783\nGating Epoch 9, Train Loss: 1.1284\nExpert 0 Epoch 1, Train Loss: 8.3226\nExpert 0 Epoch 2, Train Loss: 8.6080\nExpert 0 Epoch 3, Train Loss: 7.2992\nExpert 0 Epoch 4, Train Loss: 7.7756\nExpert 0 Epoch 5, Train Loss: 8.4865\nExpert 0 Epoch 6, Train Loss: 7.8952\nExpert 1 Epoch 1, Train Loss: 8.6131\nExpert 1 Epoch 2, Train Loss: 5.9880\nExpert 1 Epoch 3, Train Loss: 5.6021\nExpert 1 Epoch 4, Train Loss: 5.5244\nExpert 1 Epoch 5, Train Loss: 5.3978\nExpert 1 Epoch 6, Train Loss: 5.3135\nExpert 2 Epoch 1, Train Loss: 9.1075\nExpert 2 Epoch 2, Train Loss: 8.8960\nExpert 2 Epoch 3, Train Loss: 9.2101\nExpert 2 Epoch 4, Train Loss: 9.3131\nExpert 2 Epoch 5, Train Loss: 9.1056\nExpert 2 Epoch 6, Train Loss: 8.6106\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 21:53:03,600] Trial 14 finished with value: -0.49393436113993333 and parameters: {'gating_lr': 0.0006863260181887924, 'gating_epochs': 9, 'expert_lr': 0.0004388065864122191, 'expert_epochs': 6}. Best is trial 5 with value: -0.28928737640380864.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.1278\nGating Epoch 2, Train Loss: 1.0568\nGating Epoch 3, Train Loss: 1.0186\nGating Epoch 4, Train Loss: 1.1151\nGating Epoch 5, Train Loss: 1.0359\nGating Epoch 6, Train Loss: 1.0413\nGating Epoch 7, Train Loss: 1.1252\nGating Epoch 8, Train Loss: 1.1745\nExpert 0 Epoch 1, Train Loss: 9.3577\nExpert 0 Epoch 2, Train Loss: 7.6722\nExpert 0 Epoch 3, Train Loss: 7.7286\nExpert 0 Epoch 4, Train Loss: 8.5534\nExpert 0 Epoch 5, Train Loss: 7.7976\nExpert 0 Epoch 6, Train Loss: 8.6076\nExpert 0 Epoch 7, Train Loss: 7.7441\nExpert 0 Epoch 8, Train Loss: 8.0094\nExpert 0 Epoch 9, Train Loss: 7.9515\nExpert 1 Epoch 1, Train Loss: 9.5000\nExpert 1 Epoch 2, Train Loss: 8.2520\nExpert 1 Epoch 3, Train Loss: 6.4228\nExpert 1 Epoch 4, Train Loss: 6.2376\nExpert 1 Epoch 5, Train Loss: 5.7721\nExpert 1 Epoch 6, Train Loss: 5.6864\nExpert 1 Epoch 7, Train Loss: 5.7890\nExpert 1 Epoch 8, Train Loss: 5.6950\nExpert 1 Epoch 9, Train Loss: 5.7203\nExpert 2 Epoch 1, Train Loss: 8.3122\nExpert 2 Epoch 2, Train Loss: 9.3213\nExpert 2 Epoch 3, Train Loss: 8.3153\nExpert 2 Epoch 4, Train Loss: 8.9583\nExpert 2 Epoch 5, Train Loss: 8.8103\nExpert 2 Epoch 6, Train Loss: 8.8007\nExpert 2 Epoch 7, Train Loss: 9.0465\nExpert 2 Epoch 8, Train Loss: 8.9924\nExpert 2 Epoch 9, Train Loss: 8.1423\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 21:53:41,018] Trial 15 finished with value: -0.5010258356730144 and parameters: {'gating_lr': 0.00015337804958992412, 'gating_epochs': 8, 'expert_lr': 9.798445358342019e-05, 'expert_epochs': 9}. Best is trial 5 with value: -0.28928737640380864.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.0588\nGating Epoch 2, Train Loss: 0.9437\nGating Epoch 3, Train Loss: 1.0575\nGating Epoch 4, Train Loss: 1.1194\nGating Epoch 5, Train Loss: 1.0354\nGating Epoch 6, Train Loss: 1.0459\nGating Epoch 7, Train Loss: 1.0716\nGating Epoch 8, Train Loss: 1.1385\nGating Epoch 9, Train Loss: 1.2019\nGating Epoch 10, Train Loss: 1.1667\nExpert 0 Epoch 1, Train Loss: 8.4275\nExpert 0 Epoch 2, Train Loss: 8.0243\nExpert 0 Epoch 3, Train Loss: 7.7832\nExpert 0 Epoch 4, Train Loss: 7.9420\nExpert 0 Epoch 5, Train Loss: 7.7846\nExpert 0 Epoch 6, Train Loss: 8.3761\nExpert 0 Epoch 7, Train Loss: 8.4376\nExpert 0 Epoch 8, Train Loss: 8.1266\nExpert 1 Epoch 1, Train Loss: 9.0074\nExpert 1 Epoch 2, Train Loss: 5.7064\nExpert 1 Epoch 3, Train Loss: 5.5980\nExpert 1 Epoch 4, Train Loss: 5.3785\nExpert 1 Epoch 5, Train Loss: 5.3279\nExpert 1 Epoch 6, Train Loss: 5.1897\nExpert 1 Epoch 7, Train Loss: 5.0970\nExpert 1 Epoch 8, Train Loss: 4.9211\nExpert 2 Epoch 1, Train Loss: 9.2033\nExpert 2 Epoch 2, Train Loss: 8.4840\nExpert 2 Epoch 3, Train Loss: 8.3865\nExpert 2 Epoch 4, Train Loss: 8.9457\nExpert 2 Epoch 5, Train Loss: 9.0272\nExpert 2 Epoch 6, Train Loss: 8.6306\nExpert 2 Epoch 7, Train Loss: 7.9008\nExpert 2 Epoch 8, Train Loss: 8.8117\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-04-15 21:54:16,491] Trial 16 finished with value: -0.08947065671284993 and parameters: {'gating_lr': 0.0002121563783901395, 'gating_epochs': 10, 'expert_lr': 0.0008240674624446471, 'expert_epochs': 8}. Best is trial 16 with value: -0.08947065671284993.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Gating Epoch 1, Train Loss: 1.2795\nGating Epoch 2, Train Loss: 1.1124\nGating Epoch 3, Train Loss: 1.0275\nGating Epoch 4, Train Loss: 1.0027\nGating Epoch 5, Train Loss: 1.2186\nGating Epoch 6, Train Loss: 1.2887\nGating Epoch 7, Train Loss: 1.1691\nGating Epoch 8, Train Loss: 1.1721\nGating Epoch 9, Train Loss: 1.2138\nGating Epoch 10, Train Loss: 1.2679\nExpert 0 Epoch 1, Train Loss: 8.8489\nExpert 0 Epoch 2, Train Loss: 7.8500\nExpert 0 Epoch 3, Train Loss: 8.3981\nExpert 0 Epoch 4, Train Loss: 8.5941\nExpert 0 Epoch 5, Train Loss: 8.1092\nExpert 0 Epoch 6, Train Loss: 8.2189\nExpert 0 Epoch 7, Train Loss: 7.7611\nExpert 0 Epoch 8, Train Loss: 8.7595\nExpert 1 Epoch 1, Train Loss: 8.1021\nExpert 1 Epoch 2, Train Loss: 5.9502\nExpert 1 Epoch 3, Train Loss: 5.5623\nExpert 1 Epoch 4, Train Loss: 5.4477\nExpert 1 Epoch 5, Train Loss: 5.3773\nExpert 1 Epoch 6, Train Loss: 5.1596\nExpert 1 Epoch 7, Train Loss: 5.1114\nExpert 1 Epoch 8, Train Loss: 5.0824\nExpert 2 Epoch 1, Train Loss: 8.7942\nExpert 2 Epoch 2, Train Loss: 8.9943\nExpert 2 Epoch 3, Train Loss: 9.7125\nExpert 2 Epoch 4, Train Loss: 8.6888\nExpert 2 Epoch 5, Train Loss: 8.7540\nExpert 2 Epoch 6, Train Loss: 9.4215\nExpert 2 Epoch 7, Train Loss: 9.3484\nExpert 2 Epoch 8, Train Loss: 8.4311\n","output_type":"stream"},{"name":"stderr","text":"[W 2025-04-15 21:54:52,807] Trial 17 failed with parameters: {'gating_lr': 8.249007070428141e-05, 'gating_epochs': 10, 'expert_lr': 0.0009826494734997409, 'expert_epochs': 8} because of the following error: RuntimeError('[enforce fail at inline_container.cc:603] . unexpected pos 224245888 vs 224245776').\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 850, in save\n    _save(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1114, in _save\n    zip_file.write_record(name, storage, num_bytes)\nRuntimeError: [enforce fail at inline_container.cc:778] . PytorchStreamWriter failed writing file data/86: file write failed\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/3267924050.py\", line 177, in objective\n    torch.save(expert_model.state_dict(), expert_path)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 849, in save\n    with _open_zipfile_writer(f) as opened_zipfile:\n  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 690, in __exit__\n    self.file_like.write_end_of_file()\nRuntimeError: [enforce fail at inline_container.cc:603] . unexpected pos 224245888 vs 224245776\n[W 2025-04-15 21:54:52,815] Trial 17 failed with value None.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    851\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1113\u001b[0m             \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:778] . PytorchStreamWriter failed writing file data/86: file write failed","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/684648242.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nBenchmarking Top 5 Configurations:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/3267924050.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mexpert_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'expert_{expert_id}.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mexperts_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexpert_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpert_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mexpert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m             _save(\n\u001b[1;32m    851\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:603] . unexpected pos 224245888 vs 224245776"],"ename":"RuntimeError","evalue":"[enforce fail at inline_container.cc:603] . unexpected pos 224245888 vs 224245776","output_type":"error"}],"execution_count":5}]}